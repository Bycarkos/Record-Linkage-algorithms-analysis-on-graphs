{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "governing-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "\n",
    "LEFT, ROPE, RIGHT = range(3)\n",
    "\n",
    "def correlated_ttest_MC(x, rope, runs=1,  nsamples=50000):\n",
    "    \"\"\"\n",
    "    See correlated_ttest module for explanations\n",
    "    \"\"\"\n",
    "    if x.ndim == 2:\n",
    "        x = x[:, 1] - x[:, 0]\n",
    "    diff=x\n",
    "    n = len(diff)\n",
    "    nfolds = n / runs\n",
    "    x = np.mean(diff)\n",
    "    # Nadeau's and Bengio's corrected variance\n",
    "    var = np.var(diff, ddof=1) * (1 / n + 1 / (nfolds - 1))\n",
    "    if var == 0:\n",
    "        return int(x < rope), int(-rope <= x <= rope), int(rope < x)\n",
    "    \n",
    "    return x+np.sqrt(var)*np.random.standard_t( n - 1, nsamples)\n",
    "                                  \n",
    "    \n",
    "\n",
    "## Correlated t-test\n",
    "def correlated_ttest(x, rope, runs=1, verbose=False, names=('C1', 'C2')):\n",
    "    import scipy.stats as stats\n",
    "    \"\"\"\n",
    "    Compute correlated t-test\n",
    " \n",
    "    The function uses the Bayesian interpretation of the p-value and returns\n",
    "    the probabilities the difference are below `-rope`, within `[-rope, rope]`\n",
    "    and above the `rope`. For details, see `A Bayesian approach for comparing\n",
    "    cross-validated algorithms on multiple data sets\n",
    "    <http://link.springer.com/article/10.1007%2Fs10994-015-5486-z>`_,\n",
    "    G. Corani and A. Benavoli, Mach Learning 2015.\n",
    " \n",
    "    |\n",
    "    The test assumes that the classifiers were evaluated using cross\n",
    "    validation. The number of folds is determined from the length of the vector\n",
    "    of differences, as `len(diff) / runs`. The variance includes a correction\n",
    "    for underestimation of variance due to overlapping training sets, as\n",
    "    described in `Inference for the Generalization Error\n",
    "    <http://link.springer.com/article/10.1023%2FA%3A1024068626366>`_,\n",
    "    C. Nadeau and Y. Bengio, Mach Learning 2003.)\n",
    " \n",
    "    |\n",
    "    Args:\n",
    "    x (array): a vector of differences or a 2d array with pairs of scores.\n",
    "    rope (float): the width of the rope  \n",
    "    runs (int): number of repetitions of cross validation (default: 1)\n",
    "    return: probablities (tuple) that differences are below -rope, within rope or\n",
    "        above rope\n",
    "    \"\"\"\n",
    "    if x.ndim == 2:\n",
    "        x = x[:, 1] - x[:, 0]\n",
    "    diff=x\n",
    "    n = len(diff)\n",
    "    nfolds = n / runs\n",
    "    x = np.mean(diff)\n",
    "    # Nadeau's and Bengio's corrected variance\n",
    "    var = np.var(diff, ddof=1) * (1 / n + 1 / (nfolds - 1))\n",
    "    if var == 0:\n",
    "        return int(x < rope), int(-rope <= x <= rope), int(rope < x)\n",
    "    pr = 1-stats.t.cdf(rope, n - 1, x, np.sqrt(var))\n",
    "    pl = stats.t.cdf(-rope, n - 1, x, np.sqrt(var))\n",
    "    pe=1-pl-pr\n",
    "    if verbose:\n",
    "        print('P({c1} > {c2}) = {pl}, P(rope) = {pe}, P({c2} > {c1}) = {pr}'.\n",
    "              format(c1=names[0], c2=names[1], pl=pl, pe=pe, pr=pr))\n",
    "    return pl, pe, pr\n",
    "    \n",
    "## SIGN TEST\n",
    "def signtest_MC(x, rope, prior_strength=1, prior_place=ROPE, nsamples=50000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (array): a vector of differences or a 2d array with pairs of scores.\n",
    "        rope (float): the width of the rope  \n",
    "        prior_strength (float): prior strength (default: 1)\n",
    "        prior_place (LEFT, ROPE or RIGHT): the region to which the prior is\n",
    "            assigned (default: ROPE)\n",
    "        nsamples (int): the number of Monte Carlo samples\n",
    "    \n",
    "    Returns:\n",
    "        2-d array with rows corresponding to samples and columns to\n",
    "        probabilities `[p_left, p_rope, p_right]`\n",
    "    \"\"\"\n",
    "    if prior_strength < 0:\n",
    "        raise ValueError('Prior strength must be nonegative')\n",
    "    if nsamples < 0:\n",
    "        raise ValueError('Number of samples must be a positive integer')\n",
    "    if rope < 0:\n",
    "        raise ValueError('Rope must be a positive number')\n",
    " \n",
    "    if x.ndim == 2:\n",
    "        x = x[:, 1] - x[:, 0]\n",
    "    nleft = sum(x < -rope)\n",
    "    nright = sum(x > rope)\n",
    "    nrope = len(x) - nleft - nright\n",
    "    alpha = np.array([nleft, nrope, nright], dtype=float)\n",
    "    alpha += 0.0001  # for numerical stability\n",
    "    alpha[prior_place] += prior_strength\n",
    "    return np.random.dirichlet(alpha, nsamples)\n",
    "\n",
    "def signtest(x, rope, prior_strength=1, prior_place=ROPE, nsamples=50000,\n",
    "             verbose=False, names=('C1', 'C2')):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (array): a vector of differences or a 2d array with pairs of scores.\n",
    "        rope (float): the width of the rope  \n",
    "        prior_strength (float): prior strength (default: 1)\n",
    "        prior_place (LEFT, ROPE or RIGHT): the region to which the prior is\n",
    "            assigned (default: ROPE)\n",
    "        nsamples (int): the number of Monte Carlo samples\n",
    "        verbose (bool): report the computed probabilities\n",
    "        names (pair of str): the names of the two classifiers\n",
    "    Returns:\n",
    "        p_left, p_rope, p_right \n",
    "    \"\"\"\n",
    "    samples = signtest_MC(x, rope, prior_strength, prior_place, nsamples)\n",
    "    \n",
    "    winners = np.argmax(samples, axis=1)\n",
    "    pl, pe, pr = np.bincount(winners, minlength=3) / len(winners)\n",
    "    if verbose:\n",
    "        print('P({c1} > {c2}) = {pl}, P(rope) = {pe}, P({c2} > {c1}) = {pr}'.\n",
    "              format(c1=names[0], c2=names[1], pl=pl, pe=pe, pr=pr))\n",
    "    return pl, pe, pr\n",
    "\n",
    "## SIGNEDRANK\n",
    "def heaviside(X):\n",
    "    Y = np.zeros(X.shape);\n",
    "    Y[np.where(X  > 0)] = 1;\n",
    "    Y[np.where(X == 0)] = 0.5;\n",
    "    return Y #1 * (x > 0)\n",
    "\n",
    "def signrank_MC(x, rope, prior_strength=0.6, prior_place=ROPE, nsamples=50000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (array): a vector of differences or a 2d array with pairs of scores.\n",
    "        rope (float): the width of the rope  \n",
    "        prior_strength (float): prior strength (default: 0.6)\n",
    "        prior_place (LEFT, ROPE or RIGHT): the region to which the prior is\n",
    "            assigned (default: ROPE)\n",
    "        nsamples (int): the number of Monte Carlo samples\n",
    "    \n",
    "    Returns:\n",
    "        2-d array with rows corresponding to samples and columns to\n",
    "        probabilities `[p_left, p_rope, p_right]`\n",
    "    \"\"\"\n",
    "    if x.ndim == 2:\n",
    "        zm = x[:, 1] - x[:, 0]\n",
    "    else:\n",
    "        zm = x\n",
    "    nm=len(zm)\n",
    "    if prior_place==ROPE:\n",
    "        z0=[0]\n",
    "    if prior_place==LEFT:\n",
    "        z0=[-float('inf')]\n",
    "    if prior_place==RIGHT:\n",
    "        z0=[float('inf')]\n",
    "    z=np.concatenate((zm,z0))\n",
    "    n=len(z)\n",
    "    z=np.transpose(np.asmatrix(z))\n",
    "    X=np.matlib.repmat(z,1,n)\n",
    "    Y=np.matlib.repmat(-np.transpose(z)+2*rope,n,1)\n",
    "    Aright = heaviside(X-Y)\n",
    "    X=np.matlib.repmat(-z,1,n)\n",
    "    Y=np.matlib.repmat(np.transpose(z)+2*rope,n,1)\n",
    "    Aleft = heaviside(X-Y)\n",
    "    alpha=np.concatenate((np.ones(nm),[prior_strength]),axis=0)\n",
    "    samples=np.zeros((nsamples,3), dtype=float)\n",
    "    for i in range(0,nsamples):\n",
    "        data = np.random.dirichlet(alpha, 1)\n",
    "        samples[i,2]=numpy.inner(np.dot(data,Aright),data)\n",
    "        samples[i,0]=numpy.inner(np.dot(data,Aleft),data)\n",
    "        samples[i,1]=1-samples[i,0]-samples[i,2]\n",
    "     \n",
    "    return samples\n",
    "\n",
    "def signrank(x, rope, prior_strength=0.6, prior_place=ROPE, nsamples=50000,\n",
    "             verbose=False, names=('C1', 'C2')):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (array): a vector of differences or a 2d array with pairs of scores.\n",
    "        rope (float): the width of the rope \n",
    "        prior_strength (float): prior strength (default: 0.6)\n",
    "        prior_place (LEFT, ROPE or RIGHT): the region to which the prior is\n",
    "            assigned (default: ROPE)\n",
    "        nsamples (int): the number of Monte Carlo samples\n",
    "        verbose (bool): report the computed probabilities\n",
    "        names (pair of str): the names of the two classifiers\n",
    "    Returns:\n",
    "        p_left, p_rope, p_right\n",
    "    \"\"\"\n",
    "    samples = signrank_MC(x, rope, prior_strength, prior_place, nsamples)\n",
    "    \n",
    "    winners = np.argmax(samples, axis=1)\n",
    "    pl, pe, pr = np.bincount(winners, minlength=3) / len(winners)\n",
    "    if verbose:\n",
    "        print('P({c1} > {c2}) = {pl}, P(rope) = {pe}, P({c2} > {c1}) = {pr}'.\n",
    "              format(c1=names[0], c2=names[1], pl=pl, pe=pe, pr=pr))\n",
    "    return pl, pe, pr\n",
    "\n",
    "\n",
    "def hierarchical(diff, rope, rho,  upperAlpha=2, lowerAlpha =1, lowerBeta = 0.01, upperBeta = 0.1,std_upper_bound=1000, verbose=False, names=('C1', 'C2') ):\n",
    "     # upperAlpha, lowerAlpha, upperBeta, lowerBeta, are the upper and lower bound for alpha and beta, which are the parameters of \n",
    "    #the  Gamma distribution used as a prior for the degress of freedom.\n",
    "    #std_upper_bound is a constant which multiplies the sample standard deviation, to set the upper limit of the prior on the\n",
    "    #standard deviation.  Posterior inferences are insensitive to this value as this is large enough, such as 100 or 1000.\n",
    "    \n",
    "    samples=hierarchical_MC(diff, rope, rho, upperAlpha, lowerAlpha, lowerBeta, upperBeta, std_upper_bound,names )\n",
    "    winners = np.argmax(samples, axis=1)\n",
    "    pl, pe, pr = np.bincount(winners, minlength=3) / len(winners)\n",
    "    if verbose:\n",
    "        print('P({c1} > {c2}) = {pl}, P(rope) = {pe}, P({c2} > {c1}) = {pr}'.\n",
    "              format(c1=names[0], c2=names[1], pl=pl, pe=pe, pr=pr))\n",
    "    return pl, pe, pr\n",
    "\n",
    "def hierarchical_MC(diff, rope, rho,   upperAlpha=2, lowerAlpha =1, lowerBeta = 0.01, upperBeta = 0.1, std_upper_bound=1000, names=('C1', 'C2') ):\n",
    "    # upperAlpha, lowerAlpha, upperBeta, lowerBeta, are the upper and lower bound for alpha and beta, which are the parameters of \n",
    "    #the  Gamma distribution used as a prior for the degress of freedom.\n",
    "    #std_upper_bound is a constant which multiplies the sample standard deviation, to set the upper limit of the prior on the\n",
    "    #standard deviation.  Posterior inferences are insensitive to this value as this is large enough, such as 100 or 1000.\n",
    "\n",
    "    import scipy.stats as stats\n",
    "    import pystan\n",
    "    #data rescaling, to have homogenous scale among all dsets\n",
    "    stdX = np.mean(np.std(diff,1)) #we scale all the data by the mean of the standard deviation of data sets\n",
    "    x = diff/stdX\n",
    "    rope=rope/stdX\n",
    "    \n",
    "    #to avoid numerical problems with zero variance\n",
    "    for i in range(0,len(x)):\n",
    "        if np.std(x[i,:])==0:\n",
    "            x[i,:]=x[i,:]+np.random.normal(0,np.min(1/1000000000,np.abs(np.mean(x[i,:])/100000000)))\n",
    "  \n",
    "    \n",
    "    #This is the Hierarchical model written in Stan\n",
    "    hierarchical_code = \"\"\"\n",
    "    /*Hierarchical Bayesian model for the analysis of competing cross-validated classifiers on multiple data sets.\n",
    "    */\n",
    "      data {\n",
    "        real deltaLow;\n",
    "        real deltaHi;\n",
    "        //bounds of the sigma of the higher-level distribution\n",
    "        real std0Low; \n",
    "        real std0Hi; \n",
    "        //bounds on the domain of the sigma of each data set\n",
    "        real stdLow; \n",
    "        real stdHi; \n",
    "        //number of results for each data set. Typically 100 (10 runs of 10-folds cv)\n",
    "        int<lower=2> Nsamples; \n",
    "        //number of data sets. \n",
    "        int<lower=1> q; \n",
    "        //difference of accuracy between the two classifier, on each fold of each data set.\n",
    "        matrix[q,Nsamples] x;\n",
    "        //correlation (1/(number of folds))\n",
    "        real rho; \n",
    "        real upperAlpha;\n",
    "        real lowerAlpha;\n",
    "        real upperBeta;\n",
    "        real lowerBeta;\n",
    "         }\n",
    "      transformed data {\n",
    "        //vector of 1s appearing in the likelihood \n",
    "        vector[Nsamples] H;\n",
    "        //vector of 0s: the mean of the mvn noise \n",
    "        vector[Nsamples] zeroMeanVec;\n",
    "        /* M is the correlation matrix of the mvn noise.\n",
    "        invM is its inverse, detM its determinant */\n",
    "        matrix[Nsamples,Nsamples] invM;\n",
    "        real detM;\n",
    "        //The determinant of M is analytically known\n",
    "        detM <- (1+(Nsamples-1)*rho)*(1-rho)^(Nsamples-1);\n",
    "        //build H and invM. They do not depend on the data.\n",
    "        for (j in 1:Nsamples){\n",
    "          zeroMeanVec[j]<-0;\n",
    "          H[j]<-1;\n",
    "          for (i in 1:Nsamples){\n",
    "            if (j==i)\n",
    "              invM[j,i]<- (1 + (Nsamples-2)*rho)*pow((1-rho),Nsamples-2);\n",
    "            else\n",
    "              invM[j,i]<- -rho * pow((1-rho),Nsamples-2);\n",
    "           }\n",
    "        }\n",
    "        /*at this point invM contains the adjugate of M.\n",
    "        we  divide it by det(M) to obtain the inverse of M.*/\n",
    "        invM <-invM/detM;\n",
    "      }\n",
    "      parameters {\n",
    "        //mean of the  hyperprior from which we sample the delta_i\n",
    "        real<lower=deltaLow,upper=deltaHi> delta0; \n",
    "        //std of the hyperprior from which we sample the delta_i\n",
    "        real<lower=std0Low,upper=std0Hi> std0;\n",
    "        //delta_i of each data set: vector of lenght q.\n",
    "        vector[q] delta;               \n",
    "        //sigma of each data set: : vector of lenght q.\n",
    "        vector<lower=stdLow,upper=stdHi>[q] sigma; \n",
    "        /* the domain of (nu - 1) starts from 0\n",
    "        and can be given a gamma prior*/\n",
    "        real<lower=0> nuMinusOne; \n",
    "        //parameters of the Gamma prior on nuMinusOne\n",
    "        real<lower=lowerAlpha,upper=upperAlpha> gammaAlpha;\n",
    "        real<lower=lowerBeta, upper=upperBeta> gammaBeta;\n",
    "      }\n",
    "     transformed parameters {\n",
    "        //degrees of freedom\n",
    "        real<lower=1> nu ;\n",
    "        /*difference between the data (x matrix) and \n",
    "        the vector of the q means.*/\n",
    "        matrix[q,Nsamples] diff; \n",
    "        vector[q] diagQuad;\n",
    "        /*vector of length q: \n",
    "        1 over the variance of each data set*/\n",
    "        vector[q] oneOverSigma2; \n",
    "        vector[q] logDetSigma;\n",
    "        vector[q] logLik;\n",
    "        //degrees of freedom\n",
    "        nu <- nuMinusOne + 1 ;\n",
    "        //1 over the variance of each data set\n",
    "        oneOverSigma2 <- rep_vector(1, q) ./ sigma;\n",
    "        oneOverSigma2 <- oneOverSigma2 ./ sigma;\n",
    "        /*the data (x) minus a matrix done as follows:\n",
    "        the delta vector (of lenght q) pasted side by side Nsamples times*/\n",
    "        diff <- x - rep_matrix(delta,Nsamples); \n",
    "        //efficient matrix computation of the likelihood.\n",
    "        diagQuad <- diagonal (quad_form (invM,diff'));\n",
    "        logDetSigma <- 2*Nsamples*log(sigma) + log(detM) ;\n",
    "        logLik <- -0.5 * logDetSigma - 0.5*Nsamples*log(6.283);  \n",
    "        logLik <- logLik - 0.5 * oneOverSigma2 .* diagQuad;\n",
    "      }\n",
    "      model {\n",
    "        /*mu0 and std0 are not explicitly sampled here.\n",
    "        Stan automatically samples them: mu0 as uniform and std0 as\n",
    "        uniform over its domain (std0Low,std0Hi).*/\n",
    "        //sampling the degrees of freedom\n",
    "        nuMinusOne ~ gamma ( gammaAlpha, gammaBeta);\n",
    "        //vectorial sampling of the delta_i of each data set\n",
    "        delta ~ student_t(nu, delta0, std0);\n",
    "        //logLik is computed in the previous block \n",
    "        increment_log_prob(sum(logLik));   \n",
    "     }\n",
    "    \"\"\"\n",
    "    datatable=x\n",
    "    std_within=np.mean(np.std(datatable,1))\n",
    "\n",
    "    Nsamples = len(datatable[0])\n",
    "    q= len(datatable)\n",
    "    if q>1:\n",
    "        std_among=np.std(np.mean(datatable,1))\n",
    "    else:\n",
    "        std_among=np.mean(np.std(datatable,1))\n",
    "\n",
    "    #Hierarchical data in Stan\n",
    "    hierachical_dat = {'x': datatable,\n",
    "                   'deltaLow' : -np.max(np.abs(datatable)),\n",
    "                   'deltaHi' : np.max(np.abs(datatable)),\n",
    "                   'stdLow' : 0,\n",
    "                   'stdHi' : std_within*std_upper_bound,\n",
    "                   'std0Low' : 0,\n",
    "                   'std0Hi' : std_among*std_upper_bound,\n",
    "                   'Nsamples' : Nsamples,\n",
    "                   'q' : q,\n",
    "                   'rho' : rho,\n",
    "                   'upperAlpha' : upperAlpha,\n",
    "                   'lowerAlpha' : lowerAlpha,\n",
    "                   'upperBeta' : upperBeta,\n",
    "                   'lowerBeta' : lowerBeta}\n",
    "\n",
    "    #Call to Stan code\n",
    "    fit = pystan.stan(model_code=hierarchical_code, data=hierachical_dat,\n",
    "                      iter=1000, chains=4)\n",
    "    \n",
    "    la = fit.extract(permuted=True)  # return a dictionary of arrays\n",
    "    mu = la['delta0']\n",
    "    stdh = la['std0']\n",
    "    nu = la['nu']\n",
    "    \n",
    "    samples=np.zeros((len(mu),3), dtype=float)\n",
    "    for i in range(0,len(mu)):\n",
    "        samples[i,2]=1-stats.t.cdf(rope, nu[i], mu[i], stdh[i])\n",
    "        samples[i,0]=stats.t.cdf(-rope, nu[i], mu[i],  stdh[i])\n",
    "        samples[i,1]=1-samples[i,0]-samples[i,2]\n",
    "     \n",
    "    return samples\n",
    "\n",
    "def plot_posterior(samples, names=('C1', 'C2')):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (array): a vector of differences or a 2d array with pairs of scores.\n",
    "        names (pair of str): the names of the two classifiers\n",
    "    Returns:\n",
    "        matplotlib.pyplot.figure\n",
    "    \"\"\"\n",
    "    return plot_simplex(samples, names)\n",
    "\n",
    "\n",
    "def plot_simplex(points, names=('C1', 'C2')):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.lines import Line2D\n",
    "    from matplotlib.pylab import rcParams\n",
    "\n",
    "    def _project(points):\n",
    "        from math import sqrt, sin, cos, pi\n",
    "        p1, p2, p3 = points.T / sqrt(3)\n",
    "        x = (p2 - p1) * cos(pi / 6) + 0.5\n",
    "        y = p3 - (p1 + p2) * sin(pi / 6) + 1 / (2 * sqrt(3))\n",
    "        return np.vstack((x, y)).T \n",
    "\n",
    "    vert0 = _project(np.array(\n",
    "        [[0.3333, 0.3333, 0.3333], [0.5, 0.5, 0], [0.5, 0, 0.5], [0, 0.5, 0.5]]))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(8, 7)  \n",
    "    \n",
    "    nl, ne, nr = np.max(points, axis=0)\n",
    "#     for i, n in enumerate((nl, ne, nr)):\n",
    "#         if n < 0.001:\n",
    "#             print(\"p{} is too small, switching to 2d plot\".format(names[::-1] + [\"rope\"]))\n",
    "#             coords = sorted(set(range(3)) - i)\n",
    "#             return plot2d(points[:, coords], labels[coords])\n",
    "\n",
    "    # triangle\n",
    "    fig.gca().add_line(\n",
    "        Line2D([0, 0.5, 1.0, 0],\n",
    "               [0, np.sqrt(3) / 2, 0, 0], color='orange'))\n",
    "    # decision lines\n",
    "    for i in (1, 2, 3):\n",
    "        fig.gca().add_line(\n",
    "            Line2D([vert0[0, 0], vert0[i, 0]],\n",
    "                   [vert0[0, 1], vert0[i, 1]], color='orange'))\n",
    "    # vertex labels\n",
    "    rcParams.update({'font.size': 16})\n",
    "    fig.gca().text(-0.08, -0.08, 'p({})'.format(names[0]), color='orange')\n",
    "    fig.gca().text(0.44, np.sqrt(3) / 2 + 0.05, 'p(rope)', color='orange')\n",
    "    fig.gca().text(1.00, -0.08, 'p({})'.format(names[1]), color='orange')\n",
    "\n",
    "    # project and draw points\n",
    "    tripts = _project(points[:, [0, 2, 1]])\n",
    "    plt.hexbin(tripts[:, 0], tripts[:, 1], mincnt=1, cmap=plt.cm.Blues_r)            \n",
    "    # Leave some padding around the triangle for vertex labels\n",
    "    fig.gca().set_xlim(-0.2, 1.2)\n",
    "    fig.gca().set_ylim(-0.2, 1.2)\n",
    "    fig.gca().axis('off')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-spencer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
